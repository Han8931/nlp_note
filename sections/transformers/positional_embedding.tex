\section{Positional Embedding}

The self-attention mechanism in transformers treats all tokens in a sequence in parallel without an inherent notion of order. This means that, by itself, self-attention is invariant to the order of input tokens. Positional encoding is introduced to inject order information so that the model can differentiate between tokens based on their positions.
\begin{itemize}
	\item \textbf{Absolute Positional Embeddings}: Each position in the sequence is assigned a unique vector. Although straightforward, these embeddings don't scale well to longer sequences and fail to capture the nuances of relative positions between tokens.
	\item \textbf{Relative Positional Embeddings}: These embeddings focus on the distance between tokens, which can improve the model’s understanding of token relationships. However, they typically introduce additional complexity into the model architecture.
\end{itemize}

\subsection{Permutation Invariance of Self-Attention}

Without positional embeddings, the transformer's self-attention would treat the input as a bag of tokens, ignoring the order entirely. The added positional embeddings break this permutation invariance by encoding the position directly into the token representation. As a result, even if the same tokens are present, the model can infer their relative order and roles in the sentence.

Imagine you have a short sentence, ``I feel good''. If you simply pass this sentence into a transformer, the model wouldn't know the order of the words. The same set of token (\ie word) embeddings could represent the sentence ``good feel I'' if no ordering information were provided.

We formulate this as follows: With a permutation matrix \( P \) of shape \( (n, n) \), the input tokens can be permuted as 
\[
X' = P X.
\]
Let's follow the same self-attention process for \( X' \):
\[
Q' = X'W_q = P X W_q = P Q,
\]
\[
K' = X'W_k = P X W_k = P K,
\]
\[
V' = X'W_v = P X W_v = P V.
\]
Then, compute the scores using \( Q' \) and \( K' \):
\[
S' = \frac{Q'(K')^T}{\sqrt{d_k}} = \frac{(P Q)(P K)^T}{\sqrt{d_k}}.
\]
   
Note that
\[
(P K)^T = K^T P^T,
\]
so
\[
S' = \frac{P Q K^T P^T}{\sqrt{d_k}} = P \, S \, P^T.
\]

The softmax is applied row-wise. 
\[
A' = \text{Softmax}(S').
\]
   
Since \( P \) and \( P^T \) are just reordering rows and columns, respectively, the attention weights $A$ are simply permuted like below:
\[
A' = P \, A \, P^T.
\]
Finally, the output for the permuted input is:
\[
\text{Attention}(X') = A' V' = (P A P^T)(P V) = P A V = P \, \text{Attention}(X).
\]
As you can see the self-attention mechanism is \textit{equivariant} to permutations. This means that if you permute the input tokens, the output is permuted in the same way. There is no mechanism in the equations above that distinguishes one ordering from another; the operations treat all tokens symmetrically.

Thus, without additional positional encodings, if you were to shuffle the tokens, the model would compute the same set of pairwise interactions—just in a different order. The structure of the equations does not provide any mechanism for the model to know that one token came before or after another.

\subsection{Sinusoidal Positional Encoding}

In a Transformer model, positional encoding vectors are added to the token (word) embeddings before the input is fed into the self-attention layers. This addition gives the model a sense of the order in the sequence, enabling it to capture the sequential relationships between tokens despite processing them in parallel.

\begin{itemize}
	\item $\text{PE}(pos, 2i) &= \sin\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$
	\item $\text{PE}(pos, 2i+1) &= \cos\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$,
\end{itemize}
where:
\begin{itemize}
	\item \( pos \): the position of the token in the sequence (starting at 0),
	\item \( i \): the index along the embedding dimension,
	\item \( d_{\text{model}} \): the total dimension of the model’s embeddings.
\end{itemize}

Let's work through a concrete example with a very small embedding dimension:
\begin{itemize}
	\item Assume \( d_{\text{model}} = 4 \).  
	\item We’ll compute the positional encoding for two positions: \( pos = 0 \) and \( pos = 1 \).
\end{itemize}

Since \( d_{\text{model}} = 4 \), we have 4 dimensions indexed \(0, 1, 2, 3\). The formulas split the dimensions into even and odd indices:

\paragraph{For \( pos = 0 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
 \[
 \text{PE}(0,0) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^0}\Bigr) = \sin(0) = 0.
 \]
\item Dimension 1 (odd index, \( i = 0 \)):
 \[
 \text{PE}(0,1) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item Dimension 2 (even index, \( i = 1 \)):
 \[
 \text{PE}(0,2) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^{0.5}}\Bigr) = \sin(0) = 0.
 \]

\item Dimension 3 (odd index, \( i = 1 \)):
 \[
 \text{PE}(0,3) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item So, the positional encoding for \( pos = 0 \) is:
\[
[0,\, 1,\, 0,\, 1].
\]
\end{itemize}

\paragraph{For \( pos = 1 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
\[
\text{PE}(1,0) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^0}\Bigr) = \sin(1) \approx 0.84147.
\]

\item Dimension 1 (odd index, \( i = 0 \)):
\[
\text{PE}(1,1) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(1) \approx 0.54030.
\]
\item Dimension 2 (even index, \( i = 1 \)):
\[
\text{PE}(1,2) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^{0.5}}\Bigr) = \sin\Bigl(\frac{1}{100}\Bigr) = \sin(0.01) \approx 0.00999983.
\]

\item Dimension 3 (odd index, \( i = 1 \)):
\[
\text{PE}(1,3) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0.01) \approx 0.99995.
\]
\item Thus, the positional encoding for \( pos = 1 \) is approximately:
\[
[0.84147,\, 0.54030,\, 0.00999983,\, 0.99995].
\]
\end{itemize}


\subsection{RoPE}

Rather than adding a positional vector to the token embeddings, \textbf{RoPE rotates the embeddings by a position-specific angle.} Think of it as ``twisting'' the embedding in space based on its position. For instance, if you have a simple two-dimensional embedding for the word ``dog'', you can imagine its vector being rotated by an angle \(\theta\) if it's the first word, \(2\theta\) if it’s the second word, and so on.

For high-dimensional embeddings (\eg in \(\mathbb{R}^d\)), RoPE divides the vector into \(d/2\) pairs (or 2D subspaces). For a token at position \(i\), denote its embedding by:
\[
\mathbf{x}_i \in \mathbb{R}^d.
\]
We partition \(\mathbf{x}_i\) into pairs:
\[
\mathbf{x}_i^{(k)} = 
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix},\quad k = 0, 1, \ldots, \frac{d}{2}-1.
\]
Subsequently, for each 2D subspace indexed by \(k\), RoPE defines a rotation angle:
\[
\theta_{i, k} = i \cdot \alpha_k,
\]
where \(\alpha_k\) is a scaling factor that typically depends on the dimension \(k\). A popular choice is:
\[
\alpha_k = \frac{1}{10000^{\frac{2k}{d}}}.
\]
This scaling mimics the frequency scaling in sinusoidal embeddings (\ie positional embedding), ensuring that different subspaces capture positional information at different granularities.

In two-dimensional geometry, any rotation by an angle \(\theta\) can be represented by a \textit{rotation matrix} \(R(\theta)\). This matrix is a standard tool in linear algebra and has the form:
\[
R(\theta) = 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}.
\]
\begin{itemize}
	\item Geometric Interpretation: The rotation matrix \(R(\theta)\) rotates any 2D vector by the angle \(\theta\) (in the counterclockwise direction) while preserving its magnitude. This property makes it ideal for encoding positional shifts—rotating a vector does not change its ``content'' (its norm) but changes its direction, thereby encoding positional information.
	\item Inherent Relative Encoding: When different positions correspond to different rotation angles, the relationship between any two positions can be captured by the difference in their rotation angles. This leads to a natural encoding of relative position without having to explicitly compute or store separate relative position vectors.
\end{itemize}
\textbf{However, Transformer embeddings are typically high-dimensional}. Thus, RoPE adopts a simple trick. They split the high dimensional vector into two halves so that each pair forms a 2D subspace. Thus, we can apply the rotation matrix.

For each 2D subspace, the rotary transformation is applied as follows. Given the subvector:
\[
\mathbf{x}_i^{(k)} = 
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix},
\]
we compute its rotated version:
\[
\text{RoPE}_i \bigl(\mathbf{x}_i^{(k)}\bigr)
= R(\theta_{i, k})\, \mathbf{x}_i^{(k)}
= \begin{pmatrix}
\cos(\theta_{i, k}) & -\sin(\theta_{i, k}) \\
\sin(\theta_{i, k}) & \cos(\theta_{i, k})
\end{pmatrix}
\begin{pmatrix}
x_{i,2k} \\
x_{i,2k+1}
\end{pmatrix}.
\]
This yields the updated coordinates:
\[
\begin{aligned}
\tilde{x}_{i, 2k} &= x_{i,2k}\cos(\theta_{i,k}) - x_{i,2k+1}\sin(\theta_{i,k}),\\[6pt]
\tilde{x}_{i, 2k+1} &= x_{i,2k+1}\cos(\theta_{i,k}) + x_{i,2k}\sin(\theta_{i,k}).
\end{aligned}
\]
After processing all \(d/2\) subspaces, we concatenate the results back into a full \(d\)-dimensional vector \(\tilde{\mathbf{x}}_i\).

In transformer architectures, RoPE is applied to both the query and the key vectors except the value vecotrs:
\[
\begin{aligned}
\tilde{\mathbf{q}}_i &= \text{RoPE}_i(\mathbf{q}_i), \\
\tilde{\mathbf{k}}_j &= \text{RoPE}_j(\mathbf{k}_j).
\end{aligned}
\]

Then, the attention score between positions \(i\) and \(j\) is calculated as:
\[
\text{Attention}(i, j) = \frac{\tilde{\mathbf{q}}_i \cdot \tilde{\mathbf{k}}_j}{\sqrt{d}}.
\]

A key property of RoPE is that the dot product between the rotated vectors can be reinterpreted to show how relative positions are encoded. In particular, one can derive that:
\[
\tilde{\mathbf{q}}_i^\top \tilde{\mathbf{k}}_j = \mathbf{q}_i^\top\, \mathbf{M}(i,j)\, \mathbf{k}_j,
\]
where \(\mathbf{M}(i,j)\) is a block diagonal matrix that encapsulates the effect of the relative positional difference \(j-i\).

Focus on one 2D subspace (indexed by \(k\)):
\begin{itemize}
	\item The query subvector at position \(i\) is rotated by \(\theta_{i,k} = i\alpha_k\).
	\item The key subvector at position \(j\) is rotated by \(\theta_{j,k} = j\alpha_k\).
\end{itemize}

The dot product in this subspace is:
\[
\tilde{\mathbf{q}}_i^{(k)\top} \tilde{\mathbf{k}}_j^{(k)} 
= \left(\mathbf{q}_i^{(k)}\right)^\top R(\theta_{i,k})^\top R(\theta_{j,k}) \, \mathbf{k}_j^{(k)}.
\]
Since the transpose of a rotation matrix is its inverse (i.e., \(R(\theta)^\top = R(-\theta)\)), we have:
\[
R(\theta_{i,k})^\top R(\theta_{j,k}) = R(-\theta_{i,k})R(\theta_{j,k}) = R\bigl(\theta_{j,k} - \theta_{i,k}\bigr).
\]
Because \(\theta_{j,k} - \theta_{i,k} = (j-i)\alpha_k\), the transformation becomes:
\[
R\bigl((j-i)\alpha_k\bigr).
\]

Repeating this for each 2D subspace results in a block diagonal matrix:
\[
\mathbf{M}(i,j) = \text{diag}\Bigl(
R\bigl((j-i)\alpha_0\bigr),\,
R\bigl((j-i)\alpha_1\bigr),\,
\ldots,\,
R\bigl((j-i)\alpha_{\frac{d}{2}-1}\bigr)
\Bigr).
\]
Each block is the 2×2 rotation matrix:
\[
R\bigl((j-i)\alpha_k\bigr)
= \begin{pmatrix}
\cos\bigl((j-i)\alpha_k\bigr) & -\sin\bigl((j-i)\alpha_k\bigr) \\
\sin\bigl((j-i)\alpha_k\bigr) & \cos\bigl((j-i)\alpha_k\bigr)
\end{pmatrix}.
\]

\begin{itemize}
	\item Relative Positional Bias:  
  The matrix \(\mathbf{M}(i,j)\) adjusts the dot product between queries and keys based on their relative positions \((j-i)\). Thus, the value vectors are not modified. Instead of explicitly adding a relative position embedding, the rotation inherently modulates the interaction between tokens.
	\item Unified Encoding: Since \(\mathbf{M}(i,j)\) is built from standard rotation matrices \(R(\theta)\), it seamlessly encodes the relative positional difference across all 2D subspaces. This results in a unified treatment where both absolute and relative positional cues are embedded into the attention calculation.
	\item Elegant Mathematical Foundation: The use of \(R(\theta)\) comes directly from classical geometry and linear algebra. It leverages the well-known properties of rotations in 2D—specifically, that rotations preserve vector norms and that the composition of rotations is itself a rotation (with the angle being the sum or difference of the individual angles). This mathematical elegance translates into an efficient and effective mechanism for positional encoding.
\end{itemize}

% \paragraph{Summary}
% \begin{itemize}
% 	\item Stability of Vectors: Adding tokens at the end of a sentence doesn't affect the vectors for words at the beginning, facilitating efficient caching.
% 	\item Preservation of Relative Positions: If two words, say ``pig'' and ``dog,'' maintain the same relative distance in different contexts, their vectors are rotated by the same amount. This ensures that the angle, and consequently the dot product between these vectors, remains constant
% \end{itemize}

% \begin{itemize}
% 	\item Rotary Positional Encoding (RoPE) rotates the token embeddings by a position-dependent angle rather than simply adding a positional vector.
% 	\item Decomposition into 2D Subspaces: The \(d\)-dimensional embedding is split into \(d/2\) pairs. For each pair (treated as a 2D vector), a rotation is applied.
% 		\tie
% 3. **Rotation Matrix \(R(\theta)\):**  
%    The standard 2D rotation matrix,
%    \[
%    R(\theta) = 
%    \begin{pmatrix}
%    \cos(\theta) & -\sin(\theta) \\
%    \sin(\theta) & \cos(\theta)
%    \end{pmatrix},
%    \]
%    rotates vectors by \(\theta\) and comes from classical geometry. It is used here because it naturally encodes a continuous positional shift.
% 4. **Position-Specific Rotation:**  
%    For token at position \(i\) in subspace \(k\), the rotation angle is \(\theta_{i, k} = i \cdot \alpha_k\), where \(\alpha_k\) scales the rotation differently for each subspace.
% 5. **Integration with Attention:**  
%    RoPE is applied to both query and key vectors. The dot product between rotated vectors can be re-expressed as:
%    \[
%    \tilde{\mathbf{q}}_i^\top \tilde{\mathbf{k}}_j = \mathbf{q}_i^\top\, \mathbf{M}(i,j)\, \mathbf{k}_j,
%    \]
%    where \(\mathbf{M}(i,j)\) is a block diagonal matrix with blocks \(R((j-i)\alpha_k)\) that capture the relative position between tokens.

% By rotating the embeddings with \(R(\theta)\), RoPE introduces a sophisticated yet efficient mechanism to encode both absolute and relative positional information, enhancing the transformer’s ability to capture the structure and nuances inherent in sequential data.
% \end{itemize}

