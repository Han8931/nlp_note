\section{Sinusoidal (Absolute) Positional Encoding}

To encode positional information to tokens, there are several desirable properties:
\begin{enumerate}
	\item Unique encoding for each position 
	\item Linear relation between two encoded positions: If we know the encoding for position $p$, it should be straightforward to compute the encoding for position $p+k$, making it easier for the model to learn positional patterns. 
	\item Generalizes to longer sequences: it should generalize outside their training distribution.
	\item Generated by a deterministic process the model can learn: This allows the model to learn the mechanism
	\item Extensible to multiple dimensions: To handle multimodality. 
\end{enumerate}

In Transformer, positional encoding vectors are added to the token (word) embeddings before the input is fed into the self-attention layers. This addition gives the model a sense of the order in the sequence, enabling it to capture the sequential relationships between tokens despite processing them in parallel. The absolute positional encoding is given by
\begin{itemize}
	\item $\text{PE}(pos, 2i) &= \sin\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$
	\item $\text{PE}(pos, 2i+1) &= \cos\Bigg(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\Bigg)$,
\end{itemize}
where:
\begin{itemize}
	\item \( pos \): the position of the token in the sequence (starting at 0),
	\item \( i \): the index along the embedding dimension,
	\item \( d_{\text{model}} \): the total dimension of the model's embeddings.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{./images/positional_embeddings/ape.png}
	\caption{The visualization of absolute positional embedding. Each dimension oscillates at a different frequency and gets a unique positional encoding.}
\end{figure}

\subsection{Example}
Let's work through a concrete example with a very small embedding dimension:
\begin{itemize}
	\item Assume \( d_{\text{model}} = 4 \).  
	\item We'll compute the positional encoding for two positions: \( pos = 0 \) and \( pos = 1 \).
\end{itemize}

Since \( d_{\text{model}} = 4 \), we have 4 dimensions indexed \(0, 1, 2, 3\). The formulas split the dimensions into even and odd indices:

\paragraph{For \( pos = 0 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
 \[
 \text{PE}(0,0) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^0}\Bigr) = \sin(0) = 0.
 \]
\item Dimension 1 (odd index, \( i = 0 \)):
 \[
 \text{PE}(0,1) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item Dimension 2 (even index, \( i = 1 \)):
 \[
 \text{PE}(0,2) = \sin\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{0}{10000^{0.5}}\Bigr) = \sin(0) = 0.
 \]

\item Dimension 3 (odd index, \( i = 1 \)):
 \[
 \text{PE}(0,3) = \cos\Bigl(\frac{0}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0) = 1.
 \]
\item So, the positional encoding for \( pos = 0 \) is:
\[
[0,\, 1,\, 0,\, 1].
\]
\end{itemize}

\paragraph{For \( pos = 1 \)}
\begin{itemize}
\item Dimension 0 (even index, \( i = 0 \)):
\[
\text{PE}(1,0) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^0}\Bigr) = \sin(1) \approx 0.84147.
\]

\item Dimension 1 (odd index, \( i = 0 \)):
\[
\text{PE}(1,1) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot0}{4}}}\Bigr) = \cos(1) \approx 0.54030.
\]
\item Dimension 2 (even index, \( i = 1 \)):
\[
\text{PE}(1,2) = \sin\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \sin\Bigl(\frac{1}{10000^{0.5}}\Bigr) = \sin\Bigl(\frac{1}{100}\Bigr) = \sin(0.01) \approx 0.00999983.
\]

\item Dimension 3 (odd index, \( i = 1 \)):
\[
\text{PE}(1,3) = \cos\Bigl(\frac{1}{10000^{\frac{2\cdot1}{4}}}\Bigr) = \cos(0.01) \approx 0.99995.
\]
\item Thus, the positional encoding for \( pos = 1 \) is approximately:
\[
[0.84147,\, 0.54030,\, 0.00999983,\, 0.99995].
\]
\end{itemize}

\subsection{Rotation Matrix Aspect}

For each frequency $\omega_i$, the sinusoidal PE at position $p$ uses a pair
\begin{align*}
	\big[\sin(\omega_i p),\ \cos(\omega_i p)\big].
\end{align*}
Think of this as a 2D vector on the unit circle with angle $\theta=\omega_i p$.

If you move from position $p$ to $p+k$, the angle increases by $\omega_i k$. The new pair is obtained by a $2\times 2$ rotation:
\begin{align*}
	\begin{bmatrix}
		\sin(\omega_i (p+k))\\
		\cos(\omega_i (p+k))
	\end{bmatrix} = 
	\underbrace{
		\begin{bmatrix}
			\cos(\omega_i k) & -\sin(\omega_i k)\\
			\sin(\omega_i k) & \ \cos(\omega_i k)
		\end{bmatrix}}_{\text{rotation by angle }\omega_i k}
	\begin{bmatrix}
		\sin(\omega_i p)\\
		\cos(\omega_i p)
	\end{bmatrix}.
\end{align*}
This is just the angle-addition identity written as a matrix multiply. The key is a relative shift $k$ acts as a linear rotation in each $\sin,\cos$ 2D subspace.

The full PE concatenates many such pairs for different $\omega_i$. Collect them into a vector
\begin{align*}
	\mathrm{PE}(p)=\big[\sin(\omega_1 p),\cos(\omega_1 p),\ \sin(\omega_2 p),\cos(\omega_2 p),\ \dots\big].
\end{align*}
A shift by $k$ is then a \textbf{block-diagonal rotation}: 
\begin{align*}
\mathrm{PE}(p+k)=
	\underbrace{\mathrm{diag}\big(R(\omega_1 k),R(\omega_2 k),\dots\big)}_{=:~R(k)}
	\ \mathrm{PE}(p),
\end{align*}
where each block $R(\omega_i k)$ is the $2\times 2$ rotation above.

\begin{itemize}
	\item Relative structure: Because $\mathrm{PE}(p+k)=R(k)\mathrm{PE}(p)$, relative offsets are linearly encoded.
	\item Extrapolation: No learned parameters; positions outside training range still lie on circles $\to$ better length extrapolation than learned embeddings.
	\item Dot-product behavior (vanilla additive PE): In a standard Transformer, we add PE to token embeddings: $x_p = e_p + \mathrm{PE}(p)$. Attention logits involve
		\begin{align*}
		  x_p^\top x_q = e_p^\top e_q + e_p^\top \mathrm{PE}(q) + \mathrm{PE}(p)^\top e_q + \mathrm{PE}(p)^\top \mathrm{PE}(q).
		\end{align*}
  The PE–PE term depends on ($cos(w_i(q-p))$) (via angle differences), which injects relative info, but cross terms mix with content.
	\item RoPE connection: RoPE takes the rotation view further by rotating $Q$ and $K$ in these 2D subspaces, making attention depend on relative positions more cleanly than simple addition.
\end{itemize}

\paragraph{Example} Pick one frequency $\omega=\tfrac{2\pi}{T}$ (period $T$).
If $T=8\Rightarrow \omega=\tfrac{\pi}{4}$, ($p=2$), ($k=3$):
\begin{align*}
	\mathrm{PE}(2)=\big[\sin(\tfrac{\pi}{2}),\cos(\tfrac{\pi}{2})\big]=[1,0].
\end{align*}
Rotation by $\omega k=\frac{3\pi}{4}$:
\begin{align*}
	R=\begin{bmatrix}
		\cos\frac{3\pi}{4}&-\sin\frac{3\pi}{4}\\
		\sin\frac{3\pi}{4}&\cos\frac{3\pi}{4}
	\end{bmatrix}=
	\begin{bmatrix}
		-\frac{\sqrt2}{2}&-\frac{\sqrt2}{2}\\
		\frac{\sqrt2}{2}&-\frac{\sqrt2}{2}
	\end{bmatrix}.
\end{align*}
Then $R[1,0]^\top = \big[-\frac{\sqrt2}{2}, \frac{\sqrt2}{2}\big]^\top$, which equals $\big[\sin(\omega(p+k)),\cos(\omega(p+k))\big]$ as expected, where $\omega(p+k) = 5\pi/4$.

\subsection{Issues of APE}

\begin{itemize}
	\item Poor length extrapolation
		\begin{itemize}
			\item \textbf{Models see a finite range of absolute indices during training}. At inference, new indices (longer contexts) map to unseen $p_i$—distribution shift.
			\item Sinusoidal APE helps a bit (positions are computed, not learned), but periodicity can cause aliasing at very long lengths and still doesn't give a clean relative bias.
		\end{itemize}
	\item Translation non-equivariance
		\begin{itemize}
			\item If you shift the entire sequence by $+k$, absolute indices change, so the same local pattern appears under different $p_i$. The model must relearn invariances that are naturally relative (\eg the next token depends on the previous few tokens, regardless of where they are).
		\end{itemize}
	\item Polluting the semantic information of token embeddings by adding PE. 
	\item No direct encoding of relative distance
		\begin{itemize}
			\item With APE, attention scores don't decompose nicely into a term that is a function of ($i - j$). The model can learn to approximate relative behavior, but it's indirect and fragile.
		\end{itemize}
	\item Fixed or awkward max length
		\begin{itemize}
			\item Learned APE needs a table up to some $L_{max}$. Going beyond it requires interpolation or ad-hoc extension; both can degrade quality.
			\item Even sinusoidal forms often require careful frequency choices and still degrade for out-of-distribution lengths.
		\end{itemize}
	\item Streaming / chunking pain: 
		\begin{itemize}
			\item For long-form or streaming inference, resetting positions per chunk changes absolute indices and can cause mismatches when stitching attention across chunks. Extra engineering (position remapping) is needed.
		\end{itemize}
\end{itemize}

