\section{NTK-Aware Scaled RoPE}

RoPE encodes positions by rotating each query/key vector by angles that grow with token index. Those angles come from a bank of sinusoidal frequencies $\theta_i = b^{-2i/d}$, where ($b$) is the base, typically $10,000$, $d$ is the head dimension, and $i$ indexes each 2D subspace.

If you simply ask a model trained at, say, $4k$ tokens to read $16k$ tokens, the rotations at large positions become too fast and the model loses its sense of local distances.

\textit{NTK-aware scaling} fixes this by not squeezing positions, but by changing the RoPE base so the effective frequency bank shifts to lower frequencies, preserving the model's Neural Tangent Kernel (its training-time geometry) over a longer span. Empirically this lets you extend context (often 2â€“4$\times$) with small perplexity hit, even without fine-tuning; with light finetuning it goes further (\eg Code LLaMA). 

Let $s$ be the \textit{extension factor} (\eg going $4k$ $\to$ $16k$ means $s=4$), and let $|D|=d$ be the per-head dimension.

NTK-aware scaling replaces the RoPE base ($b$) by a larger base ($b'$):
\begin{align*}
	b'=b \cdot s^{\frac{d}{d-2}}
\end{align*}
Then you recompute RoPE frequencies as usual:
\begin{align*}
	\theta_i' = (b')^{-2i/d}\quad i=0\dots \frac{d}{2}-1.
\end{align*}

This is the \textit{Adjusted Base Frequency (ABF)} view derived from NTK theory; it leaves token indices ($m$) unchanged and only modifies the frequency bank. 

In short, the trick is to use a larger RoPE base while keeping token indices $m$ unchanged.

\subsection{Intuition}

Increasing $b$ slows the phase growth of rotations at high $i$ (\ie high frequencies), so relative angles between nearby tokens remain similar even when absolute positions are much larger. In the NTK view, that keeps the model's kernel (and thus its inductive bias) closer to what it saw in training, mitigating the usual long-range decay/aliasing of RoPE. Empirical and theoretical works tie context length capability tightly to the RoPE base; too small a base leads to superficial long-context behavior (low perplexity but poor retrieval). 

\begin{lstlisting}[language=Python]
# d_head = per-head dimension (even)
# base = 10000.0 by default
# s = target_context / train_context (e.g., 16_384/4_096 -> 4.0)

def ntk_aware_inv_freq(d_head: int, base: float, s: float):
    import math, torch
    b_prime = base * (s ** (d_head / (d_head - 2)))
    idx = torch.arange(0, d_head, 2.0)
    inv_freq = (b_prime ** (-idx / d_head))  # shape [d_head/2]
    return inv_freq  # use to build cos/sin angles as in standard RoPE
\end{lstlisting}

\section{Dynamic Scaling}

\section{Yet Another RoPE Extension (YaRN)}

