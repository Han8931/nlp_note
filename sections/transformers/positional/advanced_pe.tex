\section{Position Interpolation (PI)}

If a model is trained with a maximum context length (L) but we want to read sequences up to $L' = sL$, where $s>1$. PI \emph{compresses} absolute indices so that positions never exceed the range seen in training. Concretely, for a token at position $i$ (0-based), PI uses a \emph{scaled} index
\begin{align*}
	\hat{i} = \frac{i}{s}.
\end{align*}
RoPE angles are then computed with $\hat{i}$ instead of $i$. Using the notation from above, each 2D subspace $k$ is rotated by
\begin{align*}
	\hat{\theta}_{i,k} = \hat{i} \alpha_k =
	\frac{i}{s}\alpha_k,
	\quad
	\text{where}\quad
	\alpha_k = \frac{1}{10000^{2k/d}}
\end{align*}
Thus the PI-rotated query/key components become
\begin{align*}
	\widetilde{\mathbf{q}}^{(k)}_i=
	R\left(\frac{i}{s}\alpha_k\right)\mathbf{q}^{(k)}_i,
	\qquad
	\widetilde{\mathbf{k}}^{(k)}_j=
	R\left(\frac{j}{s}\alpha_k\right)\mathbf{k}^{(k)}_j.
\end{align*}
The per-subspace attention contribution depends on the \emph{scaled} relative offset:
\begin{align*}
	\left(\widetilde{\mathbf{q}}^{(k)}_i\right)^\top
	\left(\widetilde{\mathbf{k}}^{(k)}_j\right)=
	\left(\mathbf{q}^{(k)}_i\right)^\top
	R\left(\frac{j-i}{s},\alpha_k\right)
	\mathbf{k}^{(k)}_j.
\end{align*}
Equivalently, PI replaces $j-i$ with $(j-i)/s$ in the block-diagonal matrix $\mathbf{M}(i,j)$. In other words, \textbf{PI uniformly stretches the wavelength} in all RoPE planes by the same factor $s$.

In sum, PI squeezes the new sequence inside the original context window. Desptie its simplicity, PI has several limitations: 
\begin{itemize}
	\item It normally requires finetuning on about billion tokens.
	\item the perplexity slightly increases
\end{itemize}

\section{NTK-Aware Scaled RoPE}

Recall that RoPE encodes positions by rotating each query/key vector by angles that grow with token index. Those angles come from a bank of sinusoidal frequencies $\theta_i = b^{-2i/d}$, where $b$ is the base, typically $10,000$, $d$ is the head dimension, and $i$ indexes each 2D subspace.

During the pre-training, the inputs are squeezed into chunks of sequences of equal amounts of tokens $L$. After the pre-training, it tends to perform badly on sequences longer than $L$. Instead of training again on longer chunks, \textit{NTK-aware scaling} changes the base of RoPE. Empirically this lets you extend context (often 2â€“4$\times$) with small perplexity hit, even without fine-tuning; with light fine-tuning it goes further. 

Let $s$ be the \textit{extension factor} (\eg going $4k$ $\to$ $16k$ means $s=4$), and let $|D|=d$ be the per-head dimension. NTK-aware scaling replaces the RoPE base $b$ by a larger base $b'$ by
\begin{align*}
	b'=b \cdot s^{\frac{d}{d-2}}
\end{align*}
Then, you recompute RoPE frequencies as usual:
\begin{align*}
	\theta_i' = b'^{-2i/d}\quad i=0\dots \frac{d}{2}-1.
\end{align*}

This is the \textit{Adjusted Base Frequency (ABF)} view derived from NTK theory; it leaves token indices $m$ unchanged and only modifies the frequency bank. 

In short, the trick is to use a larger RoPE base while keeping token indices $m$ unchanged.

\subsection{Intuition}

Increasing $b$ slows the phase growth of rotations at high $i$ (\ie high frequencies), so relative angles between nearby tokens remain similar even when absolute positions are much larger. In the NTK view, that keeps the model's kernel (and thus its inductive bias) closer to what it saw in training, mitigating the usual long-range decay/aliasing of RoPE. Empirical and theoretical works tie context length capability tightly to the RoPE base; too small a base leads to superficial long-context behavior (low perplexity but poor retrieval). 

\begin{lstlisting}[language=Python]
# d_head = per-head dimension (even)
# base = 10000.0 by default
# s = target_context / train_context (e.g., 16_384/4_096 -> 4.0)

def ntk_aware_inv_freq(d_head: int, base: float, s: float):
    import math, torch
    b_prime = base * (s ** (d_head / (d_head - 2)))
    idx = torch.arange(0, d_head, 2.0)
    inv_freq = (b_prime ** (-idx / d_head))  # shape [d_head/2]
    return inv_freq  # use to build cos/sin angles as in standard RoPE
\end{lstlisting}

\section{YaRN: Yet another RoPE extensioN}

Extend context length \emph{without} over-warping local geometry. YaRN does this by \Ni \textbf{interpolating frequencies per dimension} instead of applying a single uniform scale, and \Nii optionally applying a mild \textbf{attention-temperature} adjustment; an \emph{inference-only} dynamic variant is also possible.

\subsection{Dimension-Aware (``by-parts'') Interpolation}

We define $\lambda_k$ as the wavelength of the RoPE embedding at $d$-th hidden dimension:
\begin{align*}
	\lambda_k = \frac{2\pi}{\alpha_k},
\end{align*}
where $k=0,1,\dots,d/2-1$. The wavelength describes the length of tokens needed in order for the RoPE embedding at dimension $k$ to perform a full rotation ($2\pi$).
Let $L$ be the training context and $s = L'/L$ the desired extension factor. We can define a \textit{usage ratio}, which counts how many cycles the $k$-th RoPE plane completes across the training window $L$:
\begin{align*}
r_k \triangleq \frac{L}{\lambda_k}=\frac{L\alpha_k}{2\pi}.
\end{align*}
\begin{itemize}
	\item Small $r_k$ means $\lambda_k \gg L$ (low frequency / long wave). Across the whole training window, this plane barely completes a fraction of a cycle 
	\item Large $r_k$ means $\lambda_k \ll L$ (high frequency / short wave). It oscillates many times inside $L$.
\end{itemize}
Choose thresholds $0<\alpha<\beta$ and a ramp
\begin{align*}
	\gamma (r)= 
	\begin{cases}
	0, & r < \alpha\\
	\frac{r-\alpha}{\beta-\alpha} & \alpha \leq r \leq \beta\\
	1, & r > \beta.
	\end{cases}
\end{align*}
YaRN \textbf{interpolates} each plane's frequency between ``no change'' and a PI-like scaling:
\begin{align*}
	\alpha_k^{\text{(yarn)}} =
	\underbrace{(1-\gamma(r_k))}_{\text{keep low-freq}}
	\cdot \alpha_k \, + \underbrace{\gamma(r_k)}_{\text{scale high-freq}} \cdot \frac{\alpha_k}{s}.
\end{align*}
\begin{itemize}
	\item Leaves low frequencies alone
	\item Scales high frequencies strongly
\end{itemize}
Equivalently, the angle at position $i$ becomes
\begin{align*}
	\theta^{\text{(yarn)}}_{i,k} =
	i \cdot \alpha_k^{\text{(yarn)}} =
	i\cdot\bigg[(1-\gamma(r_k))\alpha_k+\gamma(r_k)\frac{\alpha_k}{s}\bigg].
\end{align*}

Long-wave (low-frequency) planes preserve their original geometry (good for coarse structure), while short-wave planes receive stronger stretching (good for avoiding aliasing at long absolute positions). Mid bands are smoothly blended. Compared with PI, this limits over-compression of frequencies that were already gentle at the training window.

\subsection{Attention-Temperature Adjustment}

YaRN also introduces a softmax temperature $t$ to stabilize attention over longer spans:
\begin{align*}
	\mathrm{Attention}(i,j)=
	\mathrm{softmax}\Big(\frac{\widetilde{\mathbf{q}}_i^\top \widetilde{\mathbf{k}}_j}{t\sqrt{d}}\Big).
\end{align*}
The original paper suggests to use (for $s\geq 1$):
\begin{align*}
	\sqrt{\frac{1}{t}}
	= 0.1 \ln s + 1.
\end{align*}
The $t$ decreases smoothly as the window grows. This mitigates over-sharp attention when many positions are competing at extended lengths.

\subsection{Comparison Summary}
\begin{itemize}
	\item \textbf{PI:} $i\to i/s$ (uniform across planes).
	\item \textbf{NTK-aware base change:} keep $i$ but reduce all frequencies by increasing the RoPE base (global shift). 
	\item \textbf{YaRN:} \emph{per-dimension} blending between NTK-aware and PI-like scaling with mild attention temperature.
\end{itemize}

