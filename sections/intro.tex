\chapter{Introduction}

\section{Linguistics}
\label{sec:linguistics}

\begin{itemize}
    \item \textbf{Phonetics}: the study and systematic classification of the sounds made in spoken utterance.
	\item \textbf{Morphology}: the study of the internal structure of words and how they are formed from smaller units of meaning (\ie \textit{morphemes}).
        \begin{itemize}
            \item Example: the word \textit{unhappiness} consists of three morphemes: 
            \begin{enumerate}
                \item \textit{un-} (prefix meaning "not"),
                \item \textit{happy} (root meaning "feeling good/pleased"),
                \item \textit{-ness} (suffix turning an adjective into a noun).
            \end{enumerate}
        \end{itemize}
    \item \textbf{Syntax}: the way in which linguistic elements (such as words) are put together to form constituents (such as phrases, clauses, grammar, and word order).
        \begin{itemize}
            \item \textbf{Syntactic ambiguity}: occurs when a sentence can be interpreted in more than one way due to its structure. 
            
            \item \textit{There are an old man and woman}: This can mean either:
            \begin{enumerate}
                \item There is an old man and an old woman.
                \item There is an old man and (any) woman.
            \end{enumerate}
        \end{itemize}
    \item \textbf{Semantics}: the study of meaning in language; how words, phrases, and sentences correspond to objects, actions, and ideas in the real or imagined world.
        \begin{itemize}
            \item Includes logical relations like \textbf{entailment}: if sentence A is true, sentence B must also be true.  
            Example: \textit{John killed the wasp} $\Rightarrow$ \textit{The wasp is dead}.
        \end{itemize}
    \item \textbf{Pragmatics}: the study of how context influences the interpretation of meaning, including speaker intention, implied meaning, and conversational inference.
        \begin{itemize}
            \item \textit{Can you pass the salt?}: Semantically a question about ability, but pragmatically a request.
        \end{itemize}
\end{itemize}

\section{Language Models}
\label{sec:language_models}

\section{Pre-Training}
\label{sec:language_models}

This achievement is largely motivated by pre-training: we separate common components from many neural network-based systems, and then train them on huge amounts of unlabeled data using self-supervision. These pre-trained models serve as foundation models that can be easily adapted to different tasks via fine-tuning or prompting. As a result, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised learning for specific tasks is no longer required, and instead, we only need to adapt pre-trained foundation models.

The discussion of pre-training issues in NLP typically involves two types of problems: sequence
modeling (or sequence encoding) and sequence generation.

Optimizing $\theta$ on a pre-training task. Unlike standard learning problems in NLP, pre-training
does not assume specific downstream tasks to which the model will be applied. Instead, the goal is to train a model that can generalize across various tasks.

Applying the pre-trained model to downstream tasks. To adapt the model to these tasks, we need to adjust the parameters slightly using labeled data or prompt the model with task descriptions.


